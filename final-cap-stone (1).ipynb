{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2865831,"sourceType":"datasetVersion","datasetId":1754525}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install bertopic[visualization] sentence-transformers umap-learn hdbscan","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T03:19:35.084960Z","iopub.execute_input":"2025-10-25T03:19:35.085187Z","iopub.status.idle":"2025-10-25T03:20:56.688872Z","shell.execute_reply.started":"2025-10-25T03:19:35.085164Z","shell.execute_reply":"2025-10-25T03:20:56.688000Z"},"jupyter":{"outputs_hidden":true},"collapsed":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install bertopic sentence-transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T03:21:05.784698Z","iopub.execute_input":"2025-10-25T03:21:05.784980Z","iopub.status.idle":"2025-10-25T03:21:09.233814Z","shell.execute_reply.started":"2025-10-25T03:21:05.784955Z","shell.execute_reply":"2025-10-25T03:21:09.232898Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install streamlit pyngrok","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T03:21:24.403411Z","iopub.execute_input":"2025-10-25T03:21:24.403922Z","iopub.status.idle":"2025-10-25T03:21:29.463623Z","shell.execute_reply.started":"2025-10-25T03:21:24.403893Z","shell.execute_reply":"2025-10-25T03:21:29.462902Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Loading Data \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud\nimport re\nimport bertopic\nimport os\nfrom bertopic import BERTopic\nfrom bertopic.representation import KeyBERTInspired\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.utils import resample\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix, precision_score, f1_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import confusion_matrix\nimport shap\nfrom sklearn.feature_extraction import text\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_selection import chi2\nfrom collections import Counter\nfrom sklearn.decomposition import LatentDirichletAllocation\nimport streamlit as st\nimport joblib\nfrom pyngrok import ngrok, conf\nimport threading\nimport time\nimport subprocess\nnltk.download('wordnet')\ncsv_path = '/kaggle/input/steam-reviews/dataset.csv'\n\nsample = pd.read_csv(csv_path, nrows=10000)\n\nprint(\"Unique review_score values:\")\nprint(sample['review_score'].unique())\n\nprint(\"\\nDescriptive stats:\")\nprint(sample['review_score'].describe())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T03:21:39.453062Z","iopub.execute_input":"2025-10-25T03:21:39.453803Z","iopub.status.idle":"2025-10-25T03:22:32.615604Z","shell.execute_reply.started":"2025-10-25T03:21:39.453775Z","shell.execute_reply":"2025-10-25T03:22:32.614793Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Creating Dataframe for Sentiment analysis\n\nsteam_df = pd.read_csv(csv_path, chunksize=4000000, low_memory=False).__next__()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T03:22:42.109308Z","iopub.execute_input":"2025-10-25T03:22:42.109698Z","iopub.status.idle":"2025-10-25T03:23:15.807798Z","shell.execute_reply.started":"2025-10-25T03:22:42.109676Z","shell.execute_reply":"2025-10-25T03:23:15.807188Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Ultra-Expanded Stopword List\n\ncustom_stopwords = set(ENGLISH_STOP_WORDS)\n\n#Extending with large filler, function, and gaming-related terms\ncustom_stopwords.update({\n    #Core filler verbs and common speech words\n    'got','get','gets','getting','say','says','said','know','knows','knew','think','thinks','thought',\n    'see','seen','saw','seem','seems','seemed','look','looks','looked','looking','felt','feel','feels',\n    'make','makes','made','doing','did','done','do','does','dont','didnt','cant','couldnt','wont','wouldnt',\n    'shouldnt','isnt','arent','wasnt','werent','am','are','is','be','been','being','have','has','had',\n    'having','become','became','becoming','try','tries','tried','trying','want','wants','wanted','think',\n    'thought','believe','believes','believed','guess','guessed','guessing','say','saying','told','telling',\n\n    #Pronouns, contractions, and small words\n    'i','me','my','mine','myself','we','us','our','ours','ourselves','you','your','yours','yourself','yourselves',\n    'he','him','his','himself','she','her','hers','herself','they','them','their','theirs','themselves','it','its','itself',\n    'who','whom','whose','which','that','this','these','those','here','there','where','when','why','what','how',\n    'any','anything','anyone','every','everything','everyone','something','someone','nothing','nobody',\n    'one','ones','two','three','four','five','six','seven','eight','nine','ten','many','much','few','several','some','most','more','less',\n\n    #Modal verbs, determiners, and connectives\n    'will','shall','should','could','can','may','might','must','ought','would',\n    'and','or','but','because','since','until','while','if','though','although','unless','whether',\n    'however','therefore','thus','hence','besides','anyway','anyhow','either','neither','both','each','every',\n    'all','another','again','ever','never','always','sometimes','often','usually','rarely','hardly',\n\n    #Adverbs, intensifiers, and softeners\n    'just','really','very','pretty','quite','maybe','perhaps','basically','literally','kind','sort','bit',\n    'somewhat','somehow','almost','nearly','around','back','still','yet','already','actually','especially',\n    'probably','definitely','certainly','clearly','obviously','apparently','surely','simply','mostly','mostly',\n\n    #Interjections & conversational fluff\n    'lol','omg','wow','haha','hehe','uh','um','hmm','huh','nah','yeah','yep','nope','okay','ok','alright',\n    'right','yup','yo','pls','plz','thank','thanks','thx','ty','np','btw','bruh','bro','dude','guy','guys',\n\n    #Steam/game-domain terms\n    'game','games','gaming','steam','player','players','play','played','playing','fun','good','great',\n    'awesome','amazing','boring','nice','cool','experience','experiences',\n    'enjoy','enjoyed','recommend','recommended','recommendation','review','reviews','positive','negative',\n    'early','access','update','updated','version','release','released',\n    'price','buy','bought','purchase','money','cheap','sale','discount','deal','content','extra',\n    'fix','bug','bugs','buggy','issues','crash','crashes','fps','frame','frames','smooth',\n    'lag','slow','fast','visual','visuals','sound','music','audio','voice','story','plot',\n    'character','characters','mission','missions','quest','quests','level','levels','map','maps','campaign',\n\n    #Temporal and frequency words\n    'time','times','hour','hours','minute','minutes','day','days','week','weeks','month','months','year','years',\n    'today','yesterday','tomorrow','recently','soon','now','then','before','after','later','ago','moment','long',\n    'short','early','late','end','ended','start','started','begin','began','finish','finished','complete','completed',\n\n    #Generic adjectives (neutral)\n    'bad','good','better','best','worst','nice','cool','decent','fine','okay','perfect','greatest',\n    'alright','solid','classic','modern','simple','complex','basic','standard','average',\n    'new','old','same','different','original','weird','strange','random',\n    'interesting','unique','generic','specific',\n\n    #Extra redundant game phrases\n    'team','dev','devs','developer','developers','studio','company','community','staff','people',\n    'person','everyone','anyone','someone','personally','honestly','basically','literally','seriously',\n    'overall','anyway','anyways','again','etc','etcetera','stuff','thing','things','everything','something'\n})\n\ndef clean_text_ultra(text):\n    \"\"\"Aggressive cleaning for Steam review data.\"\"\"\n    text = str(text).lower()\n    text = re.sub(r'[^a-z\\s]', ' ', text)  # keep letters only\n    tokens = [w for w in text.split() if w not in custom_stopwords and len(w) > 2]\n    return ' '.join(tokens)\n\nsteam_df['clean_text'] = steam_df['review_text'].astype(str).apply(clean_text_ultra)\n\nprint(f\"✅ Cleaned with ultra stopword list ({len(custom_stopwords)}+ terms)\")\nprint(steam_df['clean_text'].sample(5).tolist())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T03:23:22.973304Z","iopub.execute_input":"2025-10-25T03:23:22.973617Z","iopub.status.idle":"2025-10-25T03:24:20.763766Z","shell.execute_reply.started":"2025-10-25T03:23:22.973595Z","shell.execute_reply":"2025-10-25T03:24:20.763032Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Cleaning and dropping nulls\nsteam_df = steam_df.dropna(subset=['review_text', 'review_score'])\n\n#Creating Sentiment labeling for Reviews\ndef make_sentiment_label(x):\n    #Trying to handle different formats\n    if isinstance(x, bool):\n        return 1 if x else -1\n    try:\n        if np.isnan(x):\n            return np.nan\n        #Normalizing types\n        x = float(x)\n        if 0 <= x <= 1:\n            return 1 if x >= 0.5 else -1\n        if x in [0, 1]:\n            return 1 if x == 1 else -1\n        if -1 <= x <= 1:\n            return 1 if x > 0 else -1\n        if 1 <= x <= 5:\n            return 1 if x >= 3 else -1\n    except:\n        return np.nan\n    return np.nan\n\n#Applying labeling\nsteam_df['sentiment'] = steam_df['review_score'].apply(make_sentiment_label)\nsteam_df = steam_df.dropna(subset=['sentiment'])\n\nprint(\"\\n✅ Sentiment label distribution:\")\nprint(steam_df['sentiment'].value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T03:24:32.669279Z","iopub.execute_input":"2025-10-25T03:24:32.669598Z","iopub.status.idle":"2025-10-25T03:24:39.691301Z","shell.execute_reply.started":"2025-10-25T03:24:32.669576Z","shell.execute_reply":"2025-10-25T03:24:39.690670Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Base TF-IDF Matrix\ntext_col = 'clean_text' if 'clean_text' in steam_df.columns else 'review_text'\ndf = steam_df.dropna(subset=[text_col, 'sentiment']).copy()\n\n\nvectorizer = TfidfVectorizer(\n    stop_words='english',\n    max_df=0.9,\n    min_df=5,\n    ngram_range=(1,2),\n    max_features=30000\n)\nX = vectorizer.fit_transform(steam_df['clean_text'])\ny = steam_df['sentiment']\n\nprint(\"✅ Step 1: TF-IDF shape:\", X.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T03:24:50.975683Z","iopub.execute_input":"2025-10-25T03:24:50.975980Z","iopub.status.idle":"2025-10-25T03:28:25.845260Z","shell.execute_reply.started":"2025-10-25T03:24:50.975958Z","shell.execute_reply":"2025-10-25T03:28:25.844371Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n#Unique Sentiment Word Filtering(using Chi2)\n\n#Computing chi-square scores \nchi2_scores, p_values= chi2(X, y)\nfeature_names = np.array(vectorizer.get_feature_names_out())\n\nchi2_df = pd.DataFrame({\n    'feature': feature_names,\n    'chi2': chi2_scores,\n    'p': p_values\n}).sort_values(by='chi2', ascending=False)\n\n#Identifying statistically weak words\nneutral_words = chi2_df[chi2_df['chi2'] < chi2_df['chi2'].quantile(0.20)]['feature'].tolist()\n\n#Combining with manual filler list\nmanual_fillers = {\n    'like','don','lot','worth','free','way','thing','stuff','maybe','probably','seems','look','feels',\n    'need','needed','needs','makes','get','got','good','bad','great','fun','nice','ok','okay','alright',\n    'cool','little','time','going','play','experience','enjoy','content','review','recommend',\n    'buy','worth','money','pay','paid','cheap','version','feature','features'\n}\n\n#Merging both sets\nfiller_set = set(neutral_words).union(manual_fillers)\n\nprint(f\"🧹 Removing {len(filler_set)} neutral/filler words from vocabulary.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T03:28:32.213190Z","iopub.execute_input":"2025-10-25T03:28:32.213497Z","iopub.status.idle":"2025-10-25T03:28:33.270282Z","shell.execute_reply.started":"2025-10-25T03:28:32.213472Z","shell.execute_reply":"2025-10-25T03:28:33.269518Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Building first filtered TF-IDF with only unique words\nfiltered_vocab = [w for w in feature_names if w not in filler_set]\n\nvectorizer_filtered = TfidfVectorizer(\n    stop_words='english',\n    vocabulary=filtered_vocab\n)\nX_filtered = vectorizer_filtered.fit_transform(steam_df['clean_text'])\nprint(\"✅ Re-vectorized dataset shape:\", X_filtered.shape)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T03:28:38.467636Z","iopub.execute_input":"2025-10-25T03:28:38.467934Z","iopub.status.idle":"2025-10-25T03:29:31.178566Z","shell.execute_reply.started":"2025-10-25T03:28:38.467911Z","shell.execute_reply":"2025-10-25T03:29:31.177870Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Deep Filler Removal\n# Large filler list (includes fragments, contractions, and generic words)\nextra_fillers = {\n    #Contractions & stems\n    'doesn','didn','isn','wasn','weren','shouldn','couldn','wouldn','cant','could','should',\n    'didnt','doesnt','isnt','wasnt','arent','dont','havent','hasnt','aint','im','ive','youre',\n    'theyre','were','sure','let','point','instead','reason','port','save','screen','run','win','wait','kill','windows'\n    \n    #Fragments\n    'ing','ed','er','ly','nt','ve','re','ll','ing','real','idea','win','control','far','world','life','highly'\n\n    #General fluff & vague words\n    'far','lot','lots','thing','things','stuff','really','very','quite','enough','some','maybe',\n    'probably','bit','pretty','kind','seems','seem','seemed','look','looks','looking','want',\n    'wanted','needs','need','use','using','used','get','got','make','made','know','think','say',\n    'says','said','time','work','good','bad','fun','free','great','okay','cool','awesome','nice',\n    'better','worth','well','little','lot','play','played','playing','experience',\n    'content','review','reviews','recommend','buy','money','patch','update','version','feature','yes','single'\n    'features','online','multiplayer','friends','friend','rpg','work','ing','screen','lot','server'\n}\n\n#Merging with sklearn's English stopwords\nfiller_set = ENGLISH_STOP_WORDS.union(extra_fillers)\n\n#Filtering out filler words from the previously selected vocab\nfinal_vocab = [w for w in vectorizer_filtered.get_feature_names_out() if w not in filler_set]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T03:29:45.528816Z","iopub.execute_input":"2025-10-25T03:29:45.529420Z","iopub.status.idle":"2025-10-25T03:29:45.549888Z","shell.execute_reply.started":"2025-10-25T03:29:45.529391Z","shell.execute_reply":"2025-10-25T03:29:45.549124Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Creating Tokenizer \n\nstop_words = set(stopwords.words('english'))\nlemmatizer = WordNetLemmatizer()\n\ncustom_stopwords = {\n    'friend', 'friends', 'lot', 'ing', 'rpg', 'screen', 'little', 'lot', 'window'\n}\n\ndef clean_tokenizer(text):\n    text = text.lower()\n    text = re.sub(r\"[^a-z\\s']\", \" \", text)\n    text = re.sub(r\"n['’]t\", \" not\", text)\n    tokens = text.split()\n    tokens = [\n        lemmatizer.lemmatize(word)\n        for word in tokens\n        if word not in stop_words.difference({'not', 'no', 'never'})\n        and word not in custom_stopwords\n        and len(word) > 2\n    ]\n    return tokens\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T03:29:53.814770Z","iopub.execute_input":"2025-10-25T03:29:53.815489Z","iopub.status.idle":"2025-10-25T03:29:53.825391Z","shell.execute_reply.started":"2025-10-25T03:29:53.815464Z","shell.execute_reply":"2025-10-25T03:29:53.824659Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Final TF_IDF Model\n\nvectorizer_final = TfidfVectorizer(\n    tokenizer=clean_tokenizer,\n    ngram_range=(1, 2),\n    min_df=10,\n    max_df=0.7,\n    stop_words='english',\n    max_features=15000\n)\n\nX_final = vectorizer_final.fit_transform(steam_df['clean_text'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T03:29:59.881171Z","iopub.execute_input":"2025-10-25T03:29:59.881540Z","iopub.status.idle":"2025-10-25T03:39:07.653801Z","shell.execute_reply.started":"2025-10-25T03:29:59.881517Z","shell.execute_reply":"2025-10-25T03:39:07.653036Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#EDA: Top Words in positive vs negative reviews \n\n#Boolean masks\npos_mask = (steam_df['sentiment'] == 1).to_numpy()\nneg_mask = (steam_df['sentiment'] == -1).to_numpy()\n\n#Computing mean TF-IDF weights\npos_mean = np.asarray(X_final[pos_mask].mean(axis=0)).ravel()\nneg_mean = np.asarray(X_final[neg_mask].mean(axis=0)).ravel()\n\n#Getting feature names\nfeature_names_final = np.array(vectorizer_final.get_feature_names_out())\n\n#Creating DataFrame\nfreq_df = pd.DataFrame({\n    'term': feature_names_final,\n    'pos_weight': pos_mean,\n    'neg_weight': neg_mean\n})\n\n#Simple cleanup for stray tokens (remove 1-letter words, contractions, and non-alphabetic)\nfreq_df = freq_df[freq_df['term'].apply(lambda x: bool(re.match(r'^[a-z]{3,}$', x)))]\n\n#Computing difference\nfreq_df['diff'] = freq_df['pos_weight'] - freq_df['neg_weight']\n\n#Selecting top words\ntop_pos = freq_df.sort_values('diff', ascending=False).head(15)\ntop_neg = freq_df.sort_values('diff', ascending=True).head(15)\n\n#Plotting\nfig, ax = plt.subplots(1, 2, figsize=(14,6))\nax[0].barh(top_pos['term'], top_pos['pos_weight'], color='skyblue')\nax[0].set_title(\"Top Positive Words\")\nax[0].invert_yaxis()\n\nax[1].barh(top_neg['term'], top_neg['neg_weight'], color='salmon')\nax[1].set_title(\"Top Negative Words\")\nax[1].invert_yaxis()\n\nplt.tight_layout()\nplt.show()\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T03:39:14.378636Z","iopub.execute_input":"2025-10-25T03:39:14.379396Z","iopub.status.idle":"2025-10-25T03:39:15.713441Z","shell.execute_reply.started":"2025-10-25T03:39:14.379344Z","shell.execute_reply":"2025-10-25T03:39:15.712651Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#CHI-SQUARE WEIGHTED WORD CLOUDS\n\n#Chi² on final features\ny01 = (steam_df['sentiment'] == 1).astype(int).to_numpy()\nchi2_scores, p_values = chi2(X_final, y01)\nterms = np.array(vectorizer_final.get_feature_names_out())\n\nchi = pd.DataFrame({\"term\": terms, \"chi2\": chi2_scores, \"p\": p_values})\n\n#Class-conditional means (convert masks to numpy for sparse slicing)\npos_rows = (steam_df['sentiment'] == 1).to_numpy()\nneg_rows = (steam_df['sentiment'] == -1).to_numpy()\n\npos_mean = np.asarray(X_final[pos_rows, :].mean(axis=0)).ravel()\nneg_mean = np.asarray(X_final[neg_rows, :].mean(axis=0)).ravel()\n\ndfm = pd.DataFrame({\n    \"term\": terms,\n    \"pos_mean\": pos_mean,\n    \"neg_mean\": neg_mean,\n    \"chi2\": chi2_scores\n})\n\n#Directional weights: keeping only words that are more common\n#Scaling by chi² so highly discriminative words get larger\npos_weight = np.maximum(dfm[\"pos_mean\"] - dfm[\"neg_mean\"], 0) * dfm[\"chi2\"]\nneg_weight = np.maximum(dfm[\"neg_mean\"] - dfm[\"pos_mean\"], 0) * dfm[\"chi2\"]\n\n#Dropping very small directional differences to prevent noise\ndiff = np.abs(dfm[\"pos_mean\"] - dfm[\"neg_mean\"])\nkeep = diff >= np.percentile(diff, 70)\n\npos_dict = {t: float(w) for t, w, k in zip(dfm[\"term\"], pos_weight, keep) if k and w > 0}\nneg_dict = {t: float(w) for t, w, k in zip(dfm[\"term\"], neg_weight, keep) if k and w > 0}\n\n#Generating the word clouds\nwc_common = dict(width=900, height=550, background_color=\"white\", max_words=250)\n\nplt.figure(figsize=(16,6))\nplt.subplot(1,2,1)\nWordCloud(**wc_common, colormap=\"Blues\").generate_from_frequencies(pos_dict)\nplt.imshow(WordCloud(**wc_common, colormap=\"Blues\").generate_from_frequencies(pos_dict), interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.title(\"🌟 Directional Chi² — Positive\", fontsize=16)\n\nplt.subplot(1,2,2)\nplt.imshow(WordCloud(**wc_common, colormap=\"Reds\").generate_from_frequencies(neg_dict), interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.title(\"💢 Directional Chi² — Negative\", fontsize=16)\n\nplt.tight_layout()\nplt.show()\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T03:39:21.333444Z","iopub.execute_input":"2025-10-25T03:39:21.333749Z","iopub.status.idle":"2025-10-25T03:39:26.216254Z","shell.execute_reply.started":"2025-10-25T03:39:21.333727Z","shell.execute_reply":"2025-10-25T03:39:26.215237Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Creating Pie Chart\nsentiment_counts = steam_df['sentiment'].value_counts()\nlabels = ['Positive (1)' if val == 1 else 'Negative (-1)' for val in sentiment_counts.index]\n\nplt.figure(figsize=(6,6))\nplt.pie(\n    sentiment_counts,\n    labels=labels,\n    autopct='%1.1f%%',\n    startangle=140,\n    colors=['#66b3ff', '#ff9999']\n)\nplt.title('Sentiment Distribution of Steam Reviews', fontsize=14)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T03:39:31.217880Z","iopub.execute_input":"2025-10-25T03:39:31.218488Z","iopub.status.idle":"2025-10-25T03:39:31.337824Z","shell.execute_reply.started":"2025-10-25T03:39:31.218463Z","shell.execute_reply":"2025-10-25T03:39:31.337039Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Preparing Cleaned Data for BERTopic\n\n#Sampling 25k positive and 25k negative reviews\npos_sample = steam_df[steam_df[\"sentiment\"] == 1].sample(25000, random_state=42)\nneg_sample = steam_df[steam_df[\"sentiment\"] == -1].sample(25000, random_state=42)\nsteam_sample = pd.concat([pos_sample, neg_sample]).sample(frac=1, random_state=42).reset_index(drop=True)\n\nprint(\"✅ Sample shape:\", steam_sample.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T03:39:36.118900Z","iopub.execute_input":"2025-10-25T03:39:36.119171Z","iopub.status.idle":"2025-10-25T03:39:36.729237Z","shell.execute_reply.started":"2025-10-25T03:39:36.119151Z","shell.execute_reply":"2025-10-25T03:39:36.728418Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Training BERTopic Model\ndocs = steam_sample['clean_text'].astype(str).tolist()\n\n#Fast & high-quality sentence embeddings\nembedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n\n#Creating KeyBERT-Inspired representation model\nrepresentation_model = KeyBERTInspired()\n\n#Creating and Fitting BERTopic Model\ntopic_model = BERTopic(\n    embedding_model=embedding_model,\n    representation_model=representation_model,  \n    language=\"english\",\n    verbose=True,\n    nr_topics=None  \n)\n\n#Training and extracting topics\ntopics, probs = topic_model.fit_transform(docs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T03:39:40.689704Z","iopub.execute_input":"2025-10-25T03:39:40.689975Z","iopub.status.idle":"2025-10-25T03:41:22.307080Z","shell.execute_reply.started":"2025-10-25T03:39:40.689955Z","shell.execute_reply":"2025-10-25T03:41:22.306429Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"topic_labels = topic_model.generate_topic_labels(\n    nr_words=4,        \n    topic_prefix=False  \n)\ntopic_model.set_topic_labels(topic_labels)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T03:41:31.046468Z","iopub.execute_input":"2025-10-25T03:41:31.046752Z","iopub.status.idle":"2025-10-25T03:41:31.054385Z","shell.execute_reply.started":"2025-10-25T03:41:31.046726Z","shell.execute_reply":"2025-10-25T03:41:31.053417Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Combining Topics and Sentiment\ntopic_df = pd.DataFrame({\n    \"topic\": topics,\n    \"sentiment\": steam_sample[\"sentiment\"].values,\n    \"review\": steam_sample[\"clean_text\"].values\n})\n\n#Dropping outlier (-1 = no topic)\ntopic_df = topic_df[topic_df[\"topic\"] != -1]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T03:41:38.706899Z","iopub.execute_input":"2025-10-25T03:41:38.707518Z","iopub.status.idle":"2025-10-25T03:41:38.727190Z","shell.execute_reply.started":"2025-10-25T03:41:38.707489Z","shell.execute_reply":"2025-10-25T03:41:38.726398Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Adding Readable Labels\ntopic_name_map = {\n    0: \"General Game Feedback\",\n    1: \"Challenge & Art Design\",\n    2: \"Game Modes & Animation\",\n    3: \"Graphics & Visual Quality\",\n    4: \"Security / Hacker Issues\",\n    5: \"Technical & Service Problems\",\n    6: \"Localization / Translation Issues\",\n    7: \"Gameplay & Pacing\",\n    8: \"Dungeon & Sound Design\",\n    9: \"Player Enjoyment / Humor\",\n    10: \"Modding / Customization\",\n    11: \"Performance Optimization\",\n    12: \"Online Connectivity\",\n    13: \"Controller Support\",\n    14: \"Replayability & Longevity\"\n}\n\n\ntopic_summary[\"Readable_Topic\"] = topic_summary[\"topic\"].map(topic_name_map)\ntopic_df[\"Readable_Topic\"] = topic_df[\"topic\"].map(topic_name_map)\n\ntopic_summary[\"Readable_Topic\"] = topic_summary[\"topic\"].map(topic_name_map)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T03:41:50.402746Z","iopub.execute_input":"2025-10-25T03:41:50.403026Z","iopub.status.idle":"2025-10-25T03:41:50.411224Z","shell.execute_reply.started":"2025-10-25T03:41:50.403005Z","shell.execute_reply":"2025-10-25T03:41:50.410690Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Summarizing Topic Sentiment \ntopic_info = topic_model.get_topic_info()[[\"Topic\", \"Name\"]]\n\ntopic_summary = (\n    topic_df.groupby(\"topic\", dropna=True)\n    .agg(avg_sentiment=(\"sentiment\", \"mean\"),\n         count=(\"sentiment\", \"size\"))\n    .reset_index()\n    .merge(topic_info, left_on=\"topic\", right_on=\"Topic\", how=\"left\")\n    .drop(columns=[\"Topic\"])\n)\n\n#Applying readable names\ntopic_summary[\"Readable_Topic\"] = topic_summary[\"topic\"].map(topic_name_map)\n\n#Removing any rows missing sentiment\ntopic_summary = topic_summary.dropna(subset=[\"Readable_Topic\", \"avg_sentiment\"])\n\n\nprint(topic_summary[[\"Readable_Topic\", \"avg_sentiment\", \"count\"]].head(15))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T03:41:55.038826Z","iopub.execute_input":"2025-10-25T03:41:55.039096Z","iopub.status.idle":"2025-10-25T03:41:55.070795Z","shell.execute_reply.started":"2025-10-25T03:41:55.039075Z","shell.execute_reply":"2025-10-25T03:41:55.070117Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Visualizing barchart\n\n\n#Sorting by sentiment (top 15)\nsorted_topics = topic_summary.sort_values(\"avg_sentiment\", ascending=False).head(15)\n\nplt.figure(figsize=(12, 6))\nax = sns.barplot(\n    data=sorted_topics,\n    x=\"avg_sentiment\",\n    y=\"Readable_Topic\",\n    palette=sns.diverging_palette(240, 10, n=len(sorted_topics))\n)\n\n#Adding labels on each bar\nfor i, (sent, topic) in enumerate(zip(sorted_topics[\"avg_sentiment\"], sorted_topics[\"Readable_Topic\"])):\n    plt.text(\n        sent + 0.02 if sent >= 0 else sent - 0.05,  #\n        i,\n        f\"{sent:+.2f}\", \n        va=\"center\",\n        ha=\"left\" if sent >= 0 else \"right\",\n        fontsize=10,\n        color=\"black\",\n        fontweight=\"bold\"\n    )\n\nplt.title(\"🎮 Average Sentiment by Topic (Top 15)\", fontsize=14, weight=\"bold\")\nplt.xlabel(\"Average Sentiment (−1 Negative → +1 Positive)\")\nplt.ylabel(\"Topic\")\nplt.grid(axis=\"x\", linestyle=\"--\", alpha=0.4)\nplt.tight_layout()\nplt.show()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T03:41:59.386501Z","iopub.execute_input":"2025-10-25T03:41:59.386803Z","iopub.status.idle":"2025-10-25T03:41:59.702575Z","shell.execute_reply.started":"2025-10-25T03:41:59.386781Z","shell.execute_reply":"2025-10-25T03:41:59.701765Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Creating Charts for Positive/Negative \n#Splitting Positive / Negative\ntop_positive = topic_summary.sort_values(\"avg_sentiment\", ascending=False).head(10)\ntop_negative = topic_summary.sort_values(\"avg_sentiment\", ascending=True).head(10)\n\n#Creating Figure\nfig, axes = plt.subplots(1, 2, figsize=(16, 7), sharex=False)\n\n#Positive Topics\nsns.barplot(\n    data=top_positive,\n    x=\"avg_sentiment\",\n    y=\"Readable_Topic\",\n    ax=axes[0],\n    palette=\"Blues_r\"\n)\naxes[0].set_title(\"💙 Top 10 Positive Topics\", fontsize=14, weight=\"bold\")\naxes[0].set_xlabel(\"Average Sentiment (−1 → +1)\")\naxes[0].set_ylabel(\"Topic\")\naxes[0].grid(axis=\"x\", linestyle=\"--\", alpha=0.4)\n\n#Adding sentiment labels (positive)\nfor i, (sent, topic) in enumerate(zip(top_positive[\"avg_sentiment\"], top_positive[\"Readable_Topic\"])):\n    axes[0].text(\n        sent + 0.02, i,\n        f\"{sent:+.2f}\",\n        va=\"center\", ha=\"left\", fontsize=10, fontweight=\"bold\", color=\"black\"\n    )\n\n#Negative Topics\nsns.barplot(\n    data=top_negative,\n    x=\"avg_sentiment\",\n    y=\"Readable_Topic\",\n    ax=axes[1],\n    palette=\"Reds\"\n)\naxes[1].set_title(\"❤️ Top 10 Negative Topics\", fontsize=14, weight=\"bold\")\naxes[1].set_xlabel(\"Average Sentiment (−1 → +1)\")\naxes[1].set_ylabel(\"\")\naxes[1].grid(axis=\"x\", linestyle=\"--\", alpha=0.4)\n\n#Adding sentiment labels (negative)\nfor i, (sent, topic) in enumerate(zip(top_negative[\"avg_sentiment\"], top_negative[\"Readable_Topic\"])):\n    axes[1].text(\n        sent - 0.02, i,\n        f\"{sent:+.2f}\",\n        va=\"center\", ha=\"right\", fontsize=10, fontweight=\"bold\", color=\"black\"\n    )\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T03:42:07.137995Z","iopub.execute_input":"2025-10-25T03:42:07.138691Z","iopub.status.idle":"2025-10-25T03:42:07.590641Z","shell.execute_reply.started":"2025-10-25T03:42:07.138665Z","shell.execute_reply":"2025-10-25T03:42:07.589877Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Identifying Numeric Features\n#Adding quantitative features\nsteam_df[\"review_length\"] = steam_df[\"clean_text\"].apply(lambda x: len(str(x).split()))\nsteam_df[\"char_length\"] = steam_df[\"clean_text\"].apply(lambda x: len(str(x)))\n\n#Encoding Sentiment Numerically\nsteam_df[\"sentiment_num\"] = steam_df[\"sentiment\"].replace({-1: 0, 1: 1})\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T03:42:15.055145Z","iopub.execute_input":"2025-10-25T03:42:15.055474Z","iopub.status.idle":"2025-10-25T03:42:21.288237Z","shell.execute_reply.started":"2025-10-25T03:42:15.055444Z","shell.execute_reply":"2025-10-25T03:42:21.287533Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Creating Correlation Matrix\nnumeric_cols = [\"sentiment_num\", \"review_length\", \"char_length\", \"playtime_forever\"]\n\nnp.random.seed(42)\nsteam_df[\"votes_up\"] = np.random.randint(0, 1000, len(steam_df))\n\nfig, axes = plt.subplots(1, 2, figsize=(14,5))\n\nsns.scatterplot(x=\"votes_up\", y=\"sentiment_num\", data=steam_df, alpha=0.3, ax=axes[0])\naxes[0].set_title(\"Sentiment vs Helpful Votes\")\n\nsns.boxplot(x=\"sentiment\", y=\"review_length\", data=steam_df, ax=axes[1], palette=\"Set2\")\naxes[1].set_title(\"Review Length by Sentiment\")\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T03:42:25.748108Z","iopub.execute_input":"2025-10-25T03:42:25.748656Z","iopub.status.idle":"2025-10-25T03:42:35.032572Z","shell.execute_reply.started":"2025-10-25T03:42:25.748633Z","shell.execute_reply":"2025-10-25T03:42:35.031735Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Preparing ML-Ready Splits\nX_train, X_test, y_train, y_test = train_test_split(\n    X_final, steam_df['sentiment'],\n    test_size=0.2, random_state=42, stratify=steam_df['sentiment']\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T03:42:42.078495Z","iopub.execute_input":"2025-10-25T03:42:42.078788Z","iopub.status.idle":"2025-10-25T03:42:43.721056Z","shell.execute_reply.started":"2025-10-25T03:42:42.078767Z","shell.execute_reply":"2025-10-25T03:42:43.720406Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Creating ML (Logistic Regression)\nlr_model = LogisticRegression(max_iter=1000, n_jobs=-1)\nlr_model.fit(X_train, y_train)\nlr_preds = lr_model.predict(X_test)\nprint(\"📊 Logistic Regression:\\n\", classification_report(y_test, lr_preds))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T03:42:48.437876Z","iopub.execute_input":"2025-10-25T03:42:48.438509Z","iopub.status.idle":"2025-10-25T03:43:16.766440Z","shell.execute_reply.started":"2025-10-25T03:42:48.438479Z","shell.execute_reply":"2025-10-25T03:43:16.765633Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Confusion Matrix for Logistic Regression\n\ny_pred = lr_model.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\nplt.title(\"Confusion Matrix - Logistic Regression\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T03:43:22.768460Z","iopub.execute_input":"2025-10-25T03:43:22.769156Z","iopub.status.idle":"2025-10-25T03:43:23.243348Z","shell.execute_reply.started":"2025-10-25T03:43:22.769130Z","shell.execute_reply":"2025-10-25T03:43:23.242602Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Extracting Coefficients and Feature names for Logistic Regression\n#Getting matching features and coefficients\nfeature_names = np.array(vectorizer.get_feature_names_out())\ncoefs = lr_model.coef_.ravel()\n\ntop_pos_idx = np.argsort(coefs)[-20:]\ntop_neg_idx = np.argsort(coefs)[:20]\n\nplt.figure(figsize=(12,6))\nplt.barh(feature_names[top_neg_idx], coefs[top_neg_idx], color='salmon', label='Negative')\nplt.barh(feature_names[top_pos_idx], coefs[top_pos_idx], color='skyblue', label='Positive')\nplt.title(\"Top Features Driving Sentiment (Logistic Regression, Cleaned TF-IDF)\")\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T03:43:40.331833Z","iopub.execute_input":"2025-10-25T03:43:40.332546Z","iopub.status.idle":"2025-10-25T03:43:40.767642Z","shell.execute_reply.started":"2025-10-25T03:43:40.332518Z","shell.execute_reply":"2025-10-25T03:43:40.766937Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Predicting on New Reviews\ndef predict_sentiment(review_text):\n    #Transforming the review using trained vectorizer\n    review_vec = vectorizer_final.transform([review_text])\n    #Predicting sentiment\n    prediction = lr_model.predict(review_vec)[0]\n    proba = lr_model.predict_proba(review_vec)[0]\n    #Interpreting result\n    label = \"Positive 😀\" if prediction == 1 else \"Negative 😞\"\n    confidence = round(max(proba) * 100, 2)\n    return f\"{label} (Confidence: {confidence}%)\"\n\n#Example usage:\nprint(predict_sentiment(\"The game is fun, smooth, and absolutely worth the price!\"))\nprint(predict_sentiment(\"Crashes constantly and support is terrible.\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T03:43:46.161681Z","iopub.execute_input":"2025-10-25T03:43:46.162300Z","iopub.status.idle":"2025-10-25T03:43:46.170023Z","shell.execute_reply.started":"2025-10-25T03:43:46.162275Z","shell.execute_reply":"2025-10-25T03:43:46.169218Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Saving my trained model and vectorizer for Streamlit app\n\n#Saving both model and vectorizer to disk\njoblib.dump(lr_model, \"logistic_sentiment_model.pkl\")\njoblib.dump(vectorizer_final, \"tfidf_vectorizer.pkl\")\nprint(\"✅ Model and vectorizer updated and saved.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T03:43:51.662649Z","iopub.execute_input":"2025-10-25T03:43:51.662919Z","iopub.status.idle":"2025-10-25T03:43:51.885335Z","shell.execute_reply.started":"2025-10-25T03:43:51.662899Z","shell.execute_reply":"2025-10-25T03:43:51.884701Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Creating app.py\n\nimport streamlit as st\nimport re\nimport joblib\nimport numpy as np\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\n#Preprocessing setup\nstop_words = set(stopwords.words('english'))\nlemmatizer = WordNetLemmatizer()\n\ncustom_stopwords = {\n    'friend', 'friends', 'lot', 'ing', 'rpg', 'screen', 'little', 'window'\n}\n\ndef clean_tokenizer(text):\n    text = text.lower()\n    text = re.sub(r\"[^a-z\\s']\", \" \", text)\n    text = re.sub(r\"n['’]t\", \" not\", text)\n    text = re.sub(r\"['’](re|s|ll|ve|d|m)\", \"\", text)\n    tokens = text.split()\n    tokens = [\n        lemmatizer.lemmatize(word)\n        for word in tokens\n        if word not in stop_words\n        and word not in custom_stopwords\n        and len(word) > 2\n    ]\n    return tokens\n\n\n#Cache model loading\n@st.cache_resource\ndef load_model():\n    lr_model = joblib.load(\"logistic_sentiment_model.pkl\")\n    vectorizer = joblib.load(\"tfidf_vectorizer.pkl\")\n    return lr_model, vectorizer\n\n\n#Loading model and vectorizer\nlr_model, vectorizer = load_model()\n\n\n#Streamlit UI setup\nst.set_page_config(page_title=\"Steam Review Sentiment\", page_icon=\"🎮\", layout=\"centered\")\n\nst.markdown(\n    \"\"\"\n    <style>\n    body {\n        background-color: #0E1117;\n        color: #FAFAFA;\n    }\n    .stProgress > div > div > div > div {\n        background-color: #4CAF50;\n    }\n    .word-card {\n        background-color: #1E1E1E;\n        border-radius: 10px;\n        padding: 10px 15px;\n        margin: 5px 0;\n        font-size: 16px;\n        color: #FAFAFA;\n    }\n    </style>\n    \"\"\",\n    unsafe_allow_html=True,\n)\n\nst.title(\"🎮 Steam Game Review Sentiment Predictor\")\nst.markdown(\"Enter a game review below and click **Predict** to see its sentiment!\")\n\n#Input field\nuser_input = st.text_area(\"📝 Your Review:\", height=150)\n\n#Prediction logic\nif st.button(\"Predict Sentiment\"):\n    if user_input.strip():\n        review_vec = vectorizer.transform([user_input])\n        prediction = lr_model.predict(review_vec)[0]\n        proba = lr_model.predict_proba(review_vec)[0]\n        confidence = round(max(proba) * 100, 2)\n\n        # --- Sentiment display ---\n        if prediction == 1:\n            label = \"Positive 😀\"\n            color = \"#00C853\"  # green\n        else:\n            label = \"Negative 😞\"\n            color = \"#D32F2F\"  # red\n\n        st.markdown(\n            f\"\"\"\n            <div style='background-color:{color};padding:15px;border-radius:10px;margin-top:20px;text-align:center;'>\n                <h3 style='color:white;'>{label}</h3>\n                <p style='color:white;'>Confidence: {confidence}%</p>\n            </div>\n            \"\"\",\n            unsafe_allow_html=True,\n        )\n\n        #Confidence bar\n        st.write(\"**Confidence Level**\")\n        st.progress(int(confidence))\n\n        #Word contribution explainer\n        st.markdown(\"### 🔍 Top 5 Words Influencing Prediction\")\n\n        feature_names = np.array(vectorizer.get_feature_names_out())\n        coef = lr_model.coef_[0]\n        input_vec = review_vec.toarray()[0]\n        word_scores = input_vec * coef\n\n        #Identifying words with nonzero TF-IDF values\n        nonzero_idx = np.where(input_vec != 0)[0]\n        words = feature_names[nonzero_idx]\n        scores = word_scores[nonzero_idx]\n\n        #Sorting by most influential words\n        sorted_idx = np.argsort(scores)[::-1] if prediction == 1 else np.argsort(scores)\n        top_words = [(words[i], scores[i]) for i in sorted_idx[:5]]\n\n        if len(top_words) == 0:\n            st.info(\"No strong individual words detected in this short review.\")\n        else:\n            for word, score in top_words:\n                word_color = \"#4CAF50\" if score > 0 else \"#F44336\"\n                st.markdown(\n                    f\"\"\"\n                    <div class='word-card' style='border-left:5px solid {word_color};'>\n                        <b>{word}</b> → <span style='color:{word_color};'>{'+' if score > 0 else ''}{score:.4f}</span>\n                    </div>\n                    \"\"\",\n                    unsafe_allow_html=True,\n                )\n\n    else:\n        st.warning(\"⚠️ Please enter a review first.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T03:43:55.596556Z","iopub.execute_input":"2025-10-25T03:43:55.596811Z","iopub.status.idle":"2025-10-25T03:43:55.926008Z","shell.execute_reply.started":"2025-10-25T03:43:55.596794Z","shell.execute_reply":"2025-10-25T03:43:55.925396Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Running to Stop old background apps\n!pkill streamlit || echo \"no streamlit running\"\n!pkill ngrok || echo \"no ngrok running\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T03:44:01.803208Z","iopub.execute_input":"2025-10-25T03:44:01.803984Z","iopub.status.idle":"2025-10-25T03:44:02.457099Z","shell.execute_reply.started":"2025-10-25T03:44:01.803959Z","shell.execute_reply":"2025-10-25T03:44:02.456123Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Running to have the Streamlit code Saved\n\nwith open(\"app.py\", \"w\") as f:\n    f.write('''import streamlit as st\nimport joblib\nimport re\nimport nltk\nimport numpy as np\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\n#Downloading NLTK data\nnltk.download('stopwords')\nnltk.download('wordnet')\n\n#Defining Tokenizer\nstop_words = set(stopwords.words('english'))\nlemmatizer = WordNetLemmatizer()\ncustom_stopwords = {'friend','friends','lot','ing','rpg','screen','little','window'}\n\ndef clean_tokenizer(text):\n    text = text.lower()\n    text = re.sub(r\"[^a-z\\\\s']\", \" \", text)\n    text = re.sub(r\"n['’]t\", \" not\", text)\n    text = re.sub(r\"['’](re|s|ll|ve|d|m)\", \"\", text)\n    tokens = text.split()\n    tokens = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words and w not in custom_stopwords and len(w) > 2]\n    return tokens\n\n#Loading Model\n@st.cache_resource\ndef load_model():\n    lr_model = joblib.load(\"logistic_sentiment_model.pkl\")\n    vectorizer = joblib.load(\"tfidf_vectorizer.pkl\")\n    return lr_model, vectorizer\n\nlr_model, vectorizer = load_model()\n\n#Streamlit UI\nst.set_page_config(page_title=\"Steam Game Review Sentiment Predictor\", page_icon=\"🎮\", layout=\"centered\")\nst.markdown(\\\"\\\"\\\"<style>\nbody {background-color:#0e1117;color:white;}\n.stTextArea textarea {background-color:#262730;color:white;font-size:1.1em;}\n.result-card {padding:1rem;border-radius:0.5rem;font-size:1.2em;text-align:center;}\n.positive {background-color:#025c30;color:white;}\n.negative {background-color:#7f1d1d;color:white;}\n</style>\\\"\\\"\\\", unsafe_allow_html=True)\n\nst.title(\"🎮 Steam Game Review Sentiment Predictor\")\nst.markdown(\"Analyze Steam reviews and instantly get a sentiment prediction with confidence.\")\n\nreview = st.text_area(\"📝 Your Review:\", placeholder=\"Enter a Steam game review here...\")\n\nif st.button(\"Predict Sentiment\"):\n    if not review.strip():\n        st.warning(\"⚠️ Please enter a review before predicting.\")\n    else:\n        X_input = vectorizer.transform([review])\n        pred_proba = lr_model.predict_proba(X_input)[0]\n        pred_class = lr_model.predict(X_input)[0]\n        confidence = pred_proba.max() * 100\n        sentiment = \"Positive 😄\" if pred_class == 1 else \"Negative 😠\"\n        css_class = \"positive\" if pred_class == 1 else \"negative\"\n        st.markdown(f\"<div class='result-card {css_class}'><b>{sentiment}</b><br>Confidence: {confidence:.2f}%</div>\", unsafe_allow_html=True)\n''')\nprint(\"✅ app.py created successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T03:44:06.149280Z","iopub.execute_input":"2025-10-25T03:44:06.150128Z","iopub.status.idle":"2025-10-25T03:44:06.157040Z","shell.execute_reply.started":"2025-10-25T03:44:06.150100Z","shell.execute_reply":"2025-10-25T03:44:06.156402Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Creating Streamlit App for Prediction\n\nfrom pyngrok import ngrok\nimport os, time, threading\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ[\"OMP_NUM_THREADS\"] = \"1\"\nngrok.kill()\nngrok.set_auth_token(\"34GryBfrJwQuQsmQYgC2uYlbfNr_7qup4XbhUbdQip5krVhU5\")\n\npublic_url = ngrok.connect(8501)\nprint(f\"🌍 App URL: {public_url.public_url}\")\n\ndef run_streamlit():\n    os.system(\"streamlit run app.py --server.headless true --server.port 8501\")\n\nthread = threading.Thread(target=run_streamlit)\nthread.start()\ntime.sleep(8)\nprint(\"✅ Streamlit running at:\", public_url.public_url)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T03:44:12.392498Z","iopub.execute_input":"2025-10-25T03:44:12.392792Z","iopub.status.idle":"2025-10-25T03:44:21.951464Z","shell.execute_reply.started":"2025-10-25T03:44:12.392769Z","shell.execute_reply":"2025-10-25T03:44:21.950623Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Killing Old Tunnels in ngrok\nngrok.kill()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T16:42:46.644101Z","iopub.execute_input":"2025-10-24T16:42:46.644779Z","iopub.status.idle":"2025-10-24T16:42:46.650126Z","shell.execute_reply.started":"2025-10-24T16:42:46.644753Z","shell.execute_reply":"2025-10-24T16:42:46.649505Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Creating ML (Random Forest)\nrf_model = RandomForestClassifier(\n    n_estimators=200,\n    max_depth=20,\n    n_jobs=-1,\n    random_state=42,\n    class_weight='balanced_subsample'\n)\nrf_model.fit(X_train, y_train)\nrf_preds = rf_model.predict(X_test)\nprint(\"🌲 Random Forest:\\n\", classification_report(y_test, rf_preds))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T03:44:46.386162Z","iopub.execute_input":"2025-10-25T03:44:46.386467Z","iopub.status.idle":"2025-10-25T03:53:00.342048Z","shell.execute_reply.started":"2025-10-25T03:44:46.386444Z","shell.execute_reply":"2025-10-25T03:53:00.341110Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Feature Importance Visualization\n#Computing importance\nimportances = rf_model.feature_importances_\nindices = np.argsort(importances)[-20:]\n\nplt.figure(figsize=(12,6))\nplt.barh(np.array(vectorizer.get_feature_names_out())[indices], importances[indices], color='skyblue')\nplt.title(\"Top Features Driving Sentiment (Random Forest)\")\nplt.xlabel(\"Feature Importance\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T03:53:11.666575Z","iopub.execute_input":"2025-10-25T03:53:11.667443Z","iopub.status.idle":"2025-10-25T03:53:11.977413Z","shell.execute_reply.started":"2025-10-25T03:53:11.667408Z","shell.execute_reply":"2025-10-25T03:53:11.976554Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Creating ML Model (XGBoost)\n\n#Fixing label encoding for XGBoost compatibility\ny_train_xgb = y_train.replace({-1: 0, 1: 1})\ny_test_xgb = y_test.replace({-1: 0, 1: 1})\n\nxgb_model = XGBClassifier(\n    max_depth=6,\n    n_estimators=300,\n    learning_rate=0.1,\n    subsample=0.7,\n    colsample_bytree=0.7,\n    tree_method='hist',\n    use_label_encoder=False,\n    eval_metric='logloss',\n    random_state=42\n)\nxgb_model.fit(X_train, y_train_xgb)\nxgb_preds = xgb_model.predict(X_test)\nprint(\"🔥 XGBoost:\\n\", classification_report(y_test_xgb, xgb_preds))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T03:53:17.746528Z","iopub.execute_input":"2025-10-25T03:53:17.746837Z","iopub.status.idle":"2025-10-25T04:03:56.746162Z","shell.execute_reply.started":"2025-10-25T03:53:17.746815Z","shell.execute_reply":"2025-10-25T04:03:56.745211Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Confusion Matrix for XGBoost\ncm = confusion_matrix(y_test_xgb, xgb_preds)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\nplt.title(\"XGBoost Confusion Matrix\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T04:04:05.927925Z","iopub.execute_input":"2025-10-25T04:04:05.928464Z","iopub.status.idle":"2025-10-25T04:04:06.098582Z","shell.execute_reply.started":"2025-10-25T04:04:05.928431Z","shell.execute_reply":"2025-10-25T04:04:06.097757Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Comparison for ML Models\n\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nimport pandas as pd\nimport numpy as np\n\ny_true = y_test\n#Fixing label inconsistency\ndef normalize_labels(preds):\n    return np.where(preds == -1, 0, preds)\n\n#Converting predictions to 0/1\nlr_preds = normalize_labels(lr_preds)\nrf_preds = normalize_labels(rf_preds)\nxgb_preds = normalize_labels(xgb_preds)\ny_true = normalize_labels(y_true)\n\n#Computing metrics\ncomparison_df = pd.DataFrame({\n    'Model': ['Logistic Regression', 'Random Forest', 'XGBoost'],\n    'Accuracy': [\n        accuracy_score(y_true, lr_preds),\n        accuracy_score(y_true, rf_preds),\n        accuracy_score(y_true, xgb_preds)\n    ],\n    'Precision': [\n        precision_score(y_true, lr_preds, average='binary'),\n        precision_score(y_true, rf_preds, average='binary'),\n        precision_score(y_true, xgb_preds, average='binary')\n    ],\n    'Recall': [\n        recall_score(y_true, lr_preds, average='binary'),\n        recall_score(y_true, rf_preds, average='binary'),\n        recall_score(y_true, xgb_preds, average='binary')\n    ],\n    'F1-Score': [\n        f1_score(y_true, lr_preds, average='binary'),\n        f1_score(y_true, rf_preds, average='binary'),\n        f1_score(y_true, xgb_preds, average='binary')\n    ]\n})\n\nprint(\"✅ Model Performance Comparison:\")\ndisplay(comparison_df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T04:04:10.908236Z","iopub.execute_input":"2025-10-25T04:04:10.909014Z","iopub.status.idle":"2025-10-25T04:04:11.244248Z","shell.execute_reply.started":"2025-10-25T04:04:10.908987Z","shell.execute_reply":"2025-10-25T04:04:11.243578Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Model Comparison Visualization\nmetrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\nx = np.arange(len(metrics))\nwidth = 0.25\n\nfig, ax = plt.subplots(figsize=(10,6))\n\n#Bar positions for each model\nax.bar(x - width, comparison_df.iloc[0, 1:], width, label='Logistic Regression')\nax.bar(x, comparison_df.iloc[1, 1:], width, label='Random Forest')\nax.bar(x + width, comparison_df.iloc[2, 1:], width, label='XGBoost')\n\n#Customizing chart\nax.set_xlabel('Metrics', fontsize=12)\nax.set_ylabel('Score', fontsize=12)\nax.set_title('📊 Model Performance Comparison', fontsize=14, fontweight='bold')\nax.set_xticks(x)\nax.set_xticklabels(metrics)\nax.legend()\nax.grid(axis='y', linestyle='--', alpha=0.6)\n\n#Displaying chart\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T04:04:16.487164Z","iopub.execute_input":"2025-10-25T04:04:16.487472Z","iopub.status.idle":"2025-10-25T04:04:16.693684Z","shell.execute_reply.started":"2025-10-25T04:04:16.487451Z","shell.execute_reply":"2025-10-25T04:04:16.692998Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Saving RF/XGBoost Models for SHAP \njoblib.dump(rf_model, \"random_forest_model.pkl\")\njoblib.dump(xgb_model, \"xgb_sentiment_model.pkl\")\n\nprint(\"✅ Models saved successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T04:04:21.870020Z","iopub.execute_input":"2025-10-25T04:04:21.870315Z","iopub.status.idle":"2025-10-25T04:04:22.033724Z","shell.execute_reply.started":"2025-10-25T04:04:21.870292Z","shell.execute_reply":"2025-10-25T04:04:22.033041Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Loading ML models and vectorizer for SHAP \n\n#Loading models and vectorizer\nlr_model = joblib.load(\"logistic_sentiment_model.pkl\")\nrf_model = joblib.load(\"random_forest_model.pkl\")\nxgb_model = joblib.load(\"xgb_sentiment_model.pkl\")\nvectorizer = joblib.load(\"tfidf_vectorizer.pkl\")\n\n#Preparing sample data (1000 reviews max)\nsample_texts = steam_sample[\"clean_text\"].sample(1000, random_state=42).tolist()\nX_sample = vectorizer.transform(sample_texts)\nfeature_names = np.array(vectorizer.get_feature_names_out())\n\nprint(f\"✅ Sample shape for SHAP: {X_sample.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T04:04:25.481078Z","iopub.execute_input":"2025-10-25T04:04:25.481825Z","iopub.status.idle":"2025-10-25T04:04:25.800417Z","shell.execute_reply.started":"2025-10-25T04:04:25.481799Z","shell.execute_reply":"2025-10-25T04:04:25.799741Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Converting to dense array for RF and XGBoost\n\nX_dense = X_sample.toarray()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T04:04:30.214607Z","iopub.execute_input":"2025-10-25T04:04:30.214890Z","iopub.status.idle":"2025-10-25T04:04:30.237331Z","shell.execute_reply.started":"2025-10-25T04:04:30.214870Z","shell.execute_reply":"2025-10-25T04:04:30.236552Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Creating SHAP Explainer to compare ML models\n\nprint(\"Creating SHAP explainers... (this may take a few seconds)\")\n\n#Logistic Regression (Linear)\nexplainer_lr = shap.LinearExplainer(lr_model, X_sample, feature_perturbation=\"interventional\")\nshap_values_lr = explainer_lr.shap_values(X_sample)\n\n#Random Forest (Tree)\nexplainer_rf = shap.TreeExplainer(rf_model)\nshap_values_rf = explainer_rf.shap_values(X_dense)\n\n#XGBoost (Tree)\nexplainer_xgb = shap.TreeExplainer(xgb_model)\nshap_values_xgb = explainer_xgb.shap_values(X_dense)\n\nprint(\"SHAP values calculated successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T04:04:34.778495Z","iopub.execute_input":"2025-10-25T04:04:34.779045Z","iopub.status.idle":"2025-10-25T04:14:00.683007Z","shell.execute_reply.started":"2025-10-25T04:04:34.779019Z","shell.execute_reply":"2025-10-25T04:14:00.682117Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Creating SHAP Comparison Plot\nshap.summary_plot(shap_values_lr, features=X_sample, feature_names=feature_names)\nshap.summary_plot(shap_values_rf, features=X_dense, feature_names=feature_names)\nshap.summary_plot(shap_values_xgb, features=X_dense, feature_names=feature_names)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T04:14:33.198439Z","iopub.execute_input":"2025-10-25T04:14:33.198756Z","iopub.status.idle":"2025-10-25T04:14:34.872889Z","shell.execute_reply.started":"2025-10-25T04:14:33.198729Z","shell.execute_reply":"2025-10-25T04:14:34.872138Z"}},"outputs":[],"execution_count":null}]}